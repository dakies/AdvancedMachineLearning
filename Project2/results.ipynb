{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC, NuSVC\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn import FunctionSampler\n",
    "from sklearn.ensemble import IsolationForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPLIT FUNCTIONS FOR CV ##\n",
    "\n",
    "def split_df(df, n) :\n",
    "    '''\n",
    "    df: dataframe to shuffle and split\n",
    "    n: amount of to be splitted pieces\n",
    "    splits: list of dataframes (splits)\n",
    "    '''\n",
    "    # defining list with optimal cutting points\n",
    "    split_points = list(map( lambda x: int(x*len(df)/n), (list(range(1,n)))))  \n",
    "    # shuffling the df, then splitting it from split_points\n",
    "    splits = list(np.split(df.sample(frac=1), split_points))\n",
    "    return splits\n",
    "\n",
    "def make_train_and_test(splits, index) :\n",
    "    '''\n",
    "    Take splits from splitDf, and return into test set (splits[index])\n",
    "    and training set (the rest)\n",
    "    '''\n",
    "    \n",
    "    # index is zero based, so range 0-9 for 10 fold split\n",
    "    test = splits[index]\n",
    "\n",
    "    left_lst = splits[:index]\n",
    "    right_lst = splits[index+1:]\n",
    "\n",
    "    train = pd.concat(left_lst+right_lst)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.read_csv('raw/X_train.csv')\n",
    "y_train_df = pd.read_csv('raw/y_train.csv')\n",
    "X_test_df = pd.read_csv('raw/X_test.csv')\n",
    "sample_df = pd.read_csv('raw/sample.csv')\n",
    "\n",
    "train_data_df = pd.concat([X_train_df, y_train_df.drop(['id'], axis=1)], axis=1)\n",
    "columns = np.array(train_data_df.columns)\n",
    "train_data = train_data_df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classes, counts = np.unique(y_tr, return_counts=True)\n",
    "#d = {k:v for k,v in zip(classes, counts)}\n",
    "#least_count = counts.min()\n",
    "#weights = np.array([least_count/d[i] for i in y_tr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = X_train_df.drop(['id'], axis=1).to_numpy()\n",
    "y_tr = y_train_df['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_train_df, y_train_df], axis=1)\n",
    "splits = split_df(train_df, 5)\n",
    "train, test = make_train_and_test(splits, 0)\n",
    "X_tr = train.drop(['id', 'y'], axis=1).to_numpy()\n",
    "y_tr = train['y'].to_numpy()\n",
    "X_te = test.drop(['id', 'y'], axis=1).to_numpy()\n",
    "y_te = test['y'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isof(x, y):\n",
    "    model = IsolationForest()\n",
    "    y_pred = model.fit_predict(x)\n",
    "    return x[y_pred == 1], y[y_pred == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, GenericUnivariateSelect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = VarianceThreshold(threshold=0)\n",
    "X_tr = thresh.fit_transform(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, y_tr = isof(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.under_sampling import RandomUnderSampler\n",
    "#sampler = RandomUnderSampler()\n",
    "#X_tr, y_tr = sampler.fit_resample(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "#sampler = RandomOverSampler()\n",
    "#X_tr, y_tr = sampler.fit_resample(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "sampler = SMOTEENN()\n",
    "X_tr, y_tr = sampler.fit_resample(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "selection = GenericUnivariateSelect(mode='fdr', param=0.1)#, param=0.1, mode='fwe')\n",
    "X_tr = selection.fit_transform(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = StandardScaler()\n",
    "X_tr = scale.fit_transform(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te = thresh.transform(X_te)\n",
    "X_te = selection.transform(X_te)\n",
    "X_te = scale.transform(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xgb = [\n",
    "    {\n",
    "        'estimator__max_depth':[3]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial test XGBClassifier\n",
    "clf = OneVsRestClassifier(estimator = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                                colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "                                importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
    "                                max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
    "                                n_jobs=1, nthread=None, random_state=0,\n",
    "                                reg_alpha=0, reg_lambda=1, seed=None,\n",
    "                                silent=None, subsample=1, verbosity=1))\n",
    "search = GridSearchCV(clf, param_grid=param_grid_xgb, n_jobs=-1, scoring='balanced_accuracy', cv=3, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   17.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   17.8s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=OneVsRestClassifier(estimator=XGBClassifier(base_score=0.5,\n",
       "                                                                   booster='gbtree',\n",
       "                                                                   colsample_bylevel=1,\n",
       "                                                                   colsample_bynode=1,\n",
       "                                                                   colsample_bytree=1,\n",
       "                                                                   gamma=0,\n",
       "                                                                   gpu_id=None,\n",
       "                                                                   importance_type='gain',\n",
       "                                                                   interaction_constraints=None,\n",
       "                                                                   learning_rate=0.1,\n",
       "                                                                   max_delta_step=0,\n",
       "                                                                   max_depth=3,\n",
       "                                                                   min_child_weight=1,\n",
       "                                                                   missing=None,\n",
       "                                                                   monotone_constraints=None,\n",
       "                                                                   n_estimators=100,\n",
       "                                                                   n_jobs=1,\n",
       "                                                                   nthread=None,\n",
       "                                                                   num_parallel_tree=None,\n",
       "                                                                   random_state=0,\n",
       "                                                                   reg_alpha=0,\n",
       "                                                                   reg_lambda=1,\n",
       "                                                                   scale_pos_weight=None,\n",
       "                                                                   seed=None,\n",
       "                                                                   silent=None,\n",
       "                                                                   subsample=1,\n",
       "                                                                   tree_method=None,\n",
       "                                                                   validate_parameters=None,\n",
       "                                                                   verbosity=1)),\n",
       "             n_jobs=-1, param_grid=[{'estimator__max_depth': [3]}],\n",
       "             scoring='balanced_accuracy', verbose=10)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced_accuracy score:  0.6773322808354655\n",
      "Best parameters set: {'estimator__max_depth': 3}\n"
     ]
    }
   ],
   "source": [
    "print('Best', search.scoring, 'score: ', search.best_score_)\n",
    "print(\"Best parameters set:\", search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BMAC: 0.6835214675680171\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "X_te = thresh.transform(X_te)\n",
    "X_te = selection.transform(X_te)\n",
    "X_te = scale.transform(X_te)\n",
    "\n",
    "predicted = search.predict(X_te)\n",
    "test_score = balanced_accuracy_score(y_te, predicted)\n",
    "print('Test BMAC: {}'.format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_nn = [\n",
    "    {\n",
    "        'estimator__hidden_layer_sizes':[(60,)]#, tuple(150*np.ones(3, dtype=int))]\n",
    "    } # best (70,70,70)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial test NN\n",
    "clf = OneVsRestClassifier(estimator=MLPClassifier(activation='relu', alpha=1))\n",
    "search = GridSearchCV(clf, param_grid=param_grid_nn, n_jobs=-1, scoring='balanced_accuracy', cv=3, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   20.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:   20.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=OneVsRestClassifier(estimator=MLPClassifier(alpha=1)),\n",
       "             n_jobs=-1, param_grid=[{'estimator__hidden_layer_sizes': [(60,)]}],\n",
       "             scoring='balanced_accuracy', verbose=10)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best balanced_accuracy score:  0.9925861697772516\n",
      "Best parameters set: {'estimator__hidden_layer_sizes': (60,)}\n"
     ]
    }
   ],
   "source": [
    "print('Best', search.scoring, 'score: ', search.best_score_)\n",
    "print(\"Best parameters set:\", search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([18.25391976]),\n",
       " 'std_fit_time': array([1.33633071]),\n",
       " 'mean_score_time': array([0.03503831]),\n",
       " 'std_score_time': array([0.00107297]),\n",
       " 'param_estimator__hidden_layer_sizes': masked_array(data=[(60,)],\n",
       "              mask=[False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'estimator__hidden_layer_sizes': (60,)}],\n",
       " 'split0_test_score': array([0.986411]),\n",
       " 'split1_test_score': array([0.99674481]),\n",
       " 'split2_test_score': array([0.9946027]),\n",
       " 'mean_test_score': array([0.99258617]),\n",
       " 'std_test_score': array([0.00445322]),\n",
       " 'rank_test_score': array([1], dtype=int32)}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test BMAC: 0.6914304333659173\n"
     ]
    }
   ],
   "source": [
    "predicted = search.predict(X_te)\n",
    "test_score = balanced_accuracy_score(y_te, predicted)\n",
    "print('Test BMAC: {}'.format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = [\n",
    "    {\n",
    "        'estimator__min_samples_split':[2]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial test RandomForest\n",
    "clf = OneVsRestClassifier(estimator=RandomForestClassifier())\n",
    "search = GridSearchCV(clf, param_grid=param_grid_rf, n_jobs=-1, scoring='balanced_accuracy', cv=3, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best', search.scoring, 'score: ', search.best_score_)\n",
    "print(\"Best parameters set:\", search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_svc = [\n",
    "    {\n",
    "        'kernel':['rbf']\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial test SVC try 'fdr' with svc\n",
    "clf = SVC()\n",
    "search = GridSearchCV(clf, param_grid=param_grid_svc, n_jobs=-1, scoring='balanced_accuracy', cv=3, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best', search.scoring, 'score: ', search.best_score_)\n",
    "print(\"Best parameters set:\", search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = search.predict(X_te)\n",
    "test_score = balanced_accuracy_score(y_te, predicted)\n",
    "print('Test BMAC: {}'.format(test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_nusvc = [\n",
    "    {\n",
    "        'estimator__nu':[0.2,0.25,0.3,0.35]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = OneVsRestClassifier(estimator=NuSVC())\n",
    "search = GridSearchCV(clf, param_grid=param_grid_nusvc, n_jobs=-1, scoring='balanced_accuracy', cv=3, verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best', search.scoring, 'score: ', search.best_score_)\n",
    "print(\"Best parameters set:\", search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRYING NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([X_train_df, y_train_df], axis=1)\n",
    "splits = split_df(train_df, 5)\n",
    "train, test = make_train_and_test(splits, 0)\n",
    "X_tr = train.drop(['id', 'y'], axis=1).to_numpy()\n",
    "y_tr = train['y'].to_numpy()\n",
    "X_te = test.drop(['id', 'y'], axis=1).to_numpy()\n",
    "y_te = test['y'].to_numpy()\n",
    "\n",
    "thresh = VarianceThreshold(threshold=0)\n",
    "X_tr = thresh.fit_transform(X_tr)\n",
    "\n",
    "X_tr, y_tr = isof(X_tr, y_tr)\n",
    "\n",
    "#sampler = SMOTEENN()\n",
    "sampler = RandomUnderSampler()\n",
    "X_tr, y_tr = sampler.fit_resample(X_tr, y_tr)\n",
    "\n",
    "selection = GenericUnivariateSelect(mode='fwe', param=0.1)#, param=0.1)\n",
    "X_tr = selection.fit_transform(X_tr, y_tr)\n",
    "\n",
    "scale = StandardScaler()\n",
    "X_tr = scale.fit_transform(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_tr = X_train_df.drop(['id'], axis=1).to_numpy()\n",
    "#y_tr = y_train_df['y'].to_numpy()\n",
    "\n",
    "# Split into train+val and test\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X_tr, y_tr, test_size=0.1, stratify=y_tr, random_state=69)\n",
    "\n",
    "# Split train into train-val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=0.1, stratify=y_trainval, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "train_dataset = ClassifierDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).long())\n",
    "val_dataset = ClassifierDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).long())\n",
    "test_dataset = ClassifierDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list = []\n",
    "for _, t in train_dataset:\n",
    "    target_list.append(t)\n",
    "    \n",
    "target_list = torch.tensor(target_list)\n",
    "target_list = target_list[torch.randperm(len(target_list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count = [len(y_train[y_train == 0]), len(y_train[y_train == 1]), len(y_train[y_train == 2])]\n",
    "class_weights = 1./torch.tensor(class_count, dtype=torch.float) \n",
    "class_weights_all = class_weights[target_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_sampler = WeightedRandomSampler(\n",
    "    weights=class_weights_all,\n",
    "    num_samples=len(class_weights_all),\n",
    "    replacement=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 70\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.0007\n",
    "NUM_FEATURES = X_tr.shape[1]\n",
    "NUM_CLASSES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          sampler=weighted_sampler\n",
    ")\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassClassification(nn.Module):\n",
    "    def __init__(self, num_feature, num_class):\n",
    "        super(MulticlassClassification, self).__init__()\n",
    "        \n",
    "        self.layer_1 = nn.Linear(num_feature, 30)\n",
    "        #self.layer_2 = nn.Linear(20, 20)\n",
    "        #self.layer_2b = nn.Linear(256, 128)\n",
    "        #self.layer_3 = nn.Linear(128, 64)\n",
    "        \n",
    "        self.layer_out = nn.Linear(30, num_class) \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(128)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(64)\n",
    "        self.batchnorm0 = nn.BatchNorm1d(30)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #x = self.layer_1(x)\n",
    "        #x = self.batchnorm2(x)\n",
    "        #x = self.relu(x)\n",
    "        \n",
    "        #x = self.layer_3(x)\n",
    "        #x = self.batchnorm3(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_1(x)\n",
    "        #x = self.batchnorm0(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        #x = self.layer_2(x)\n",
    "        #x = self.batchnorm0(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.dropout(x)\n",
    "        \n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MulticlassClassification(\n",
      "  (layer_1): Linear(in_features=869, out_features=30, bias=True)\n",
      "  (layer_out): Linear(in_features=30, out_features=3, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (batchnorm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm3): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batchnorm0): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MulticlassClassification(num_feature = NUM_FEATURES, num_class=NUM_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "    y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)    \n",
    "    \n",
    "    correct_pred = (y_pred_tags == y_test).float()\n",
    "    acc = correct_pred.sum() / len(correct_pred)\n",
    "    \n",
    "    acc = torch.round(acc) * 100\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}\n",
    "loss_stats = {\n",
    "    'train': [],\n",
    "    \"val\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31fb860175034442863b46ef64c3f030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=70.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001: | Train Loss: 1.04000 | Val Loss: 0.94785 | Train Acc: 61.111| Val Acc: 71.429\n",
      "Epoch 002: | Train Loss: 0.94702 | Val Loss: 0.87040 | Train Acc: 55.556| Val Acc: 71.429\n",
      "Epoch 003: | Train Loss: 0.87899 | Val Loss: 0.82154 | Train Acc: 100.000| Val Acc: 71.429\n",
      "Epoch 004: | Train Loss: 0.88414 | Val Loss: 0.81991 | Train Acc: 77.778| Val Acc: 100.000\n",
      "Epoch 005: | Train Loss: 0.85153 | Val Loss: 0.79550 | Train Acc: 88.889| Val Acc: 85.714\n",
      "Epoch 006: | Train Loss: 0.82707 | Val Loss: 0.82128 | Train Acc: 88.889| Val Acc: 71.429\n",
      "Epoch 007: | Train Loss: 0.83552 | Val Loss: 0.78789 | Train Acc: 94.444| Val Acc: 100.000\n",
      "Epoch 008: | Train Loss: 0.84112 | Val Loss: 0.77135 | Train Acc: 94.444| Val Acc: 85.714\n",
      "Epoch 009: | Train Loss: 0.79953 | Val Loss: 0.76575 | Train Acc: 100.000| Val Acc: 85.714\n",
      "Epoch 010: | Train Loss: 0.79725 | Val Loss: 0.77196 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 011: | Train Loss: 0.83835 | Val Loss: 0.80262 | Train Acc: 94.444| Val Acc: 85.714\n",
      "Epoch 012: | Train Loss: 0.79897 | Val Loss: 0.75655 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 013: | Train Loss: 0.76999 | Val Loss: 0.75853 | Train Acc: 100.000| Val Acc: 85.714\n",
      "Epoch 014: | Train Loss: 0.82137 | Val Loss: 0.73808 | Train Acc: 94.444| Val Acc: 100.000\n",
      "Epoch 015: | Train Loss: 0.79533 | Val Loss: 0.73671 | Train Acc: 94.444| Val Acc: 100.000\n",
      "Epoch 016: | Train Loss: 0.79018 | Val Loss: 0.72868 | Train Acc: 94.444| Val Acc: 100.000\n",
      "Epoch 017: | Train Loss: 0.76925 | Val Loss: 0.74806 | Train Acc: 100.000| Val Acc: 85.714\n",
      "Epoch 018: | Train Loss: 0.78379 | Val Loss: 0.74868 | Train Acc: 100.000| Val Acc: 85.714\n",
      "Epoch 019: | Train Loss: 0.77203 | Val Loss: 0.73590 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 020: | Train Loss: 0.78355 | Val Loss: 0.73467 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 021: | Train Loss: 0.76768 | Val Loss: 0.72650 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 022: | Train Loss: 0.76311 | Val Loss: 0.70757 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 023: | Train Loss: 0.75325 | Val Loss: 0.70480 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 024: | Train Loss: 0.73064 | Val Loss: 0.69967 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 025: | Train Loss: 0.75939 | Val Loss: 0.70997 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 026: | Train Loss: 0.74674 | Val Loss: 0.73319 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 027: | Train Loss: 0.69521 | Val Loss: 0.71717 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 028: | Train Loss: 0.72178 | Val Loss: 0.70806 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 029: | Train Loss: 0.71109 | Val Loss: 0.68968 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 030: | Train Loss: 0.71224 | Val Loss: 0.67867 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 031: | Train Loss: 0.74991 | Val Loss: 0.67393 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 032: | Train Loss: 0.70061 | Val Loss: 0.68802 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 033: | Train Loss: 0.70976 | Val Loss: 0.67372 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 034: | Train Loss: 0.73567 | Val Loss: 0.72565 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 035: | Train Loss: 0.68164 | Val Loss: 0.69259 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 036: | Train Loss: 0.69850 | Val Loss: 0.68955 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 037: | Train Loss: 0.66598 | Val Loss: 0.66823 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 038: | Train Loss: 0.67256 | Val Loss: 0.66375 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 039: | Train Loss: 0.70024 | Val Loss: 0.66390 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 040: | Train Loss: 0.69094 | Val Loss: 0.71932 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 041: | Train Loss: 0.67788 | Val Loss: 0.65941 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 042: | Train Loss: 0.65849 | Val Loss: 0.66176 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 043: | Train Loss: 0.68238 | Val Loss: 0.66307 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 044: | Train Loss: 0.69933 | Val Loss: 0.66943 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 045: | Train Loss: 0.67451 | Val Loss: 0.67835 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 046: | Train Loss: 0.68948 | Val Loss: 0.67695 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 047: | Train Loss: 0.66737 | Val Loss: 0.65986 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 048: | Train Loss: 0.65729 | Val Loss: 0.70650 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 049: | Train Loss: 0.70945 | Val Loss: 0.67390 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 050: | Train Loss: 0.64981 | Val Loss: 0.67035 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 051: | Train Loss: 0.65297 | Val Loss: 0.66958 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 052: | Train Loss: 0.63412 | Val Loss: 0.66457 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 053: | Train Loss: 0.67293 | Val Loss: 0.69141 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 054: | Train Loss: 0.66271 | Val Loss: 0.65025 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 055: | Train Loss: 0.63788 | Val Loss: 0.65963 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 056: | Train Loss: 0.67286 | Val Loss: 0.71526 | Train Acc: 100.000| Val Acc: 85.714\n",
      "Epoch 057: | Train Loss: 0.62162 | Val Loss: 0.66356 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 058: | Train Loss: 0.64339 | Val Loss: 0.67136 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 059: | Train Loss: 0.63863 | Val Loss: 0.66832 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 060: | Train Loss: 0.66102 | Val Loss: 0.69467 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 061: | Train Loss: 0.68164 | Val Loss: 0.69228 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 062: | Train Loss: 0.62290 | Val Loss: 0.65289 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 063: | Train Loss: 0.62501 | Val Loss: 0.65531 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 064: | Train Loss: 0.62904 | Val Loss: 0.65926 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 065: | Train Loss: 0.63472 | Val Loss: 0.71587 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 066: | Train Loss: 0.63025 | Val Loss: 0.65667 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 067: | Train Loss: 0.59920 | Val Loss: 0.65458 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 068: | Train Loss: 0.59164 | Val Loss: 0.64778 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 069: | Train Loss: 0.61711 | Val Loss: 0.66846 | Train Acc: 100.000| Val Acc: 100.000\n",
      "Epoch 070: | Train Loss: 0.60528 | Val Loss: 0.66324 | Train Acc: 100.000| Val Acc: 100.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Begin training.\")\n",
    "for e in tqdm(range(1, EPOCHS+1)):\n",
    "    \n",
    "    # TRAINING\n",
    "    train_epoch_loss = 0\n",
    "    train_epoch_acc = 0\n",
    "    model.train()\n",
    "    for X_train_batch, y_train_batch in train_loader:\n",
    "        X_train_batch, y_train_batch = X_train_batch.to(device), y_train_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_train_pred = model(X_train_batch)\n",
    "        \n",
    "        train_loss = criterion(y_train_pred, y_train_batch)\n",
    "        train_acc = multi_acc(y_train_pred, y_train_batch)\n",
    "        \n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_epoch_loss += train_loss.item()\n",
    "        train_epoch_acc += train_acc.item()\n",
    "        \n",
    "        \n",
    "    # VALIDATION    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        val_epoch_loss = 0\n",
    "        val_epoch_acc = 0\n",
    "        \n",
    "        model.eval()\n",
    "        for X_val_batch, y_val_batch in val_loader:\n",
    "            X_val_batch, y_val_batch = X_val_batch.to(device), y_val_batch.to(device)\n",
    "            \n",
    "            y_val_pred = model(X_val_batch)\n",
    "                        \n",
    "            val_loss = criterion(y_val_pred, y_val_batch)\n",
    "            val_acc = multi_acc(y_val_pred, y_val_batch)\n",
    "            \n",
    "            val_epoch_loss += val_loss.item()\n",
    "            val_epoch_acc += val_acc.item()\n",
    "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
    "    loss_stats['val'].append(val_epoch_loss/len(val_loader))\n",
    "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
    "    accuracy_stats['val'].append(val_epoch_acc/len(val_loader))\n",
    "                              \n",
    "    \n",
    "    print(f'Epoch {e+0:03}: | Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(val_loader):.5f} | Train Acc: {train_epoch_acc/len(train_loader):.3f}| Val Acc: {val_epoch_acc/len(val_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for X_batch, _ in test_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_test_pred = model(X_batch)\n",
    "        y_pred_softmax = torch.log_softmax(y_test_pred, dim = 1)\n",
    "        _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
    "        y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balanced_accuracy_score(y_test, np.array(y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(960, 869)"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te = test.drop(['id', 'y'], axis=1).to_numpy()\n",
    "y_te = test['y'].to_numpy()\n",
    "X_te = thresh.transform(X_te)\n",
    "X_te = selection.transform(X_te)\n",
    "X_te = scale.fit_transform(X_te)\n",
    "\n",
    "y_pred_list = []\n",
    "y_test_pred = model(torch.Tensor(X_te))\n",
    "y_pred_softmax = torch.log_softmax(y_test_pred, dim = 1)\n",
    "_, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
    "y_pred_list.append(y_pred_tags.cpu().numpy())\n",
    "y_pred_list = [a.squeeze().tolist() for a in y_pred_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6315275354717081"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_accuracy_score(y_te, np.array(y_pred_list).reshape(960))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial test\n",
    "splits = split_df(train_data_df, 10)\n",
    "BMACs = []\n",
    "\n",
    "for i in range(10): # 10-fold cross-validation\n",
    "    train, test = make_train_and_test(splits, i)\n",
    "    X_train = train.drop(columns=['id','y']).to_numpy()\n",
    "    y_train = train['y'].to_numpy()\n",
    "    X_validate = test.drop(columns=['id','y']).to_numpy()\n",
    "    y_validate = test['y'].to_numpy()\n",
    "    \n",
    "    clf = OneVsRestClassifier(xgb.XGBClassifier(n_jobs=-1, max_depth=4))\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "            \n",
    "    y_predicted = clf.predict(X_validate)\n",
    "    current_BMAC = balanced_accuracy_score(y_validate, y_predicted)\n",
    "    BMACs.append(current_BMAC)\n",
    "    print('{i}th iteration done'.format(i=i+1))\n",
    "\n",
    "mean_BMAC = (1./len(BMACs)) * np.sum(BMACs)\n",
    "print('--------------')\n",
    "print('mean BMAC score: {m}'.format(m=mean_BMAC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial test\n",
    "splits = split_df(train_data_df, 10)\n",
    "BMACs = []\n",
    "\n",
    "for i in range(10): # 10-fold cross-validation\n",
    "    train, test = make_train_and_test(splits, i)\n",
    "    X_train = train.drop(columns=['id','y']).to_numpy()\n",
    "    y_train = train['y'].to_numpy()\n",
    "    X_validate = test.drop(columns=['id','y']).to_numpy()\n",
    "    y_validate = test['y'].to_numpy()\n",
    "    \n",
    "    clf = SVC()\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "            \n",
    "    y_predicted = clf.predict(X_validate)\n",
    "    current_BMAC = balanced_accuracy_score(y_validate, y_predicted)\n",
    "    BMACs.append(current_BMAC)\n",
    "    print('{i}th iteration done'.format(i=i+1))\n",
    "    print('----> BMAC score: {s}'.format(s=current_BMAC))\n",
    "\n",
    "mean_BMAC = (1./len(BMACs)) * np.sum(BMACs)\n",
    "print('--------------')\n",
    "print('mean BMAC score: {m}'.format(m=mean_BMAC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test the same pipeline of task1 (feature selection, outlier detection etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn import FunctionSampler\n",
    "\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINAL PIPELINE:\n",
    "### - 1) Feature selection\n",
    "### - 2) Outlier detection\n",
    "### - 3) Scale (Normalization)\n",
    "### - 4) Parameter Tuning XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2) FEATURE SELECTION ##\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "splits = split_df(train_data_df, 5)\n",
    "important_indices_dic = {}\n",
    "\n",
    "for i in range(5): # 5-fold cross-validation\n",
    "    train, test = make_train_and_test(splits, i)\n",
    "    X_train = train.drop(columns=['id','y']).to_numpy()\n",
    "    y_train = train['y'].to_numpy()\n",
    "    X_validate = test.drop(columns=['id','y']).to_numpy()\n",
    "    y_validate = test['y'].to_numpy()\n",
    "        \n",
    "    model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                                colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "                                importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
    "                                max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
    "                                n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
    "                                reg_alpha=0, reg_lambda=1, seed=None,\n",
    "                                silent=None, subsample=1, verbosity=1)\n",
    "    model.fit(X_train, y_train)\n",
    "    #feature_selector = SelectFromModel(model, prefit=True)\n",
    "    #X_new = feature_selector.transform(X_train)\n",
    "    #print(X_new.shape)\n",
    "    \n",
    "    # feature selection\n",
    "    thresholds = np.sort(model.feature_importances_)[-300:]\n",
    "    important_indices =  model.feature_importances_.argsort()[-300:]\n",
    "    for indice in important_indices:\n",
    "        if indice in important_indices_dic.keys():\n",
    "            important_indices_dic[indice] += 1\n",
    "        else:\n",
    "            important_indices_dic[indice] = 1\n",
    "            \n",
    "    print('{i}th iteration done'.format(i=i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import yaml\n",
    "    \n",
    "important_indices_dic_str = {}\n",
    "for item in important_indices_dic.items():\n",
    "    important_indices_dic_str[str(item[0])] = item[1]\n",
    "    \n",
    "with open('feature_importances.yaml', 'w') as file:\n",
    "    yaml.dump(important_indices_dic_str, file, default_flow_style=False'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('feature_importances.yaml') as file:\n",
    "    # The FullLoader parameter handles the conversion from YAML\n",
    "    # scalar values to Python the dictionary format\n",
    "    important_indices_dic_str = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = [50,70,100,150,200]\n",
    "#number_of_refined_features = [65,70,75,80,85,90]\n",
    "\n",
    "BMAC_dic = {}\n",
    "for number in number_of_features:\n",
    "    most_frequent = sorted(important_indices_dic, key=important_indices_dic.get, reverse=True)[:number]\n",
    "    drop_list = ['x{}'.format(index) for index in range(832) if index not in most_frequent]\n",
    "    selected_features = train_data_df.drop(drop_list, axis=1)\n",
    "\n",
    "    splits = split_df(selected_features, 5)\n",
    "    BMACs = []\n",
    "\n",
    "    for i in range(5): # 5-fold cross-validation\n",
    "        train, test = make_train_and_test(splits, i)\n",
    "        X_train = train.drop(columns=['id','y']).to_numpy()\n",
    "        y_train = train['y'].to_numpy()\n",
    "        X_validate = test.drop(columns=['id','y']).to_numpy()\n",
    "        y_validate = test['y'].to_numpy()\n",
    "        \n",
    "        model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                                colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "                                importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
    "                                max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
    "                                n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
    "                                reg_alpha=0, reg_lambda=1, seed=None,\n",
    "                                silent=None, subsample=1, verbosity=1)\n",
    "        eval_set = [(X_validate, y_validate)]\n",
    "        #model.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=\"rmse\", eval_set=eval_set, verbose=False)\n",
    "        model.fit(X_train, y_train)\n",
    "            \n",
    "        y_predicted = model.predict(X_validate)\n",
    "        current_BMAC = balanced_accuracy_score(y_validate, y_predicted)\n",
    "        BMACs.append(current_BMAC)\n",
    "        print('{i}th iteration done'.format(i=i+1))\n",
    "        print('----> BMAC score: {s}'.format(s=current_BMAC))\n",
    "\n",
    "    mean_BMAC = (1./len(BMACs)) * np.sum(BMACs)\n",
    "    print('------------------')\n",
    "    print('Mean BMAC score for number {n} is: {bmac}'.format(n=number, bmac=mean_BMAC))\n",
    "    BMAC_dic[number] = mean_BMAC\n",
    "print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go with 100 features\n",
    "most_frequent = sorted(important_indices_dic, key=important_indices_dic.get, reverse=True)[:100]\n",
    "drop_list = ['x{}'.format(index) for index in range(832) if index not in most_frequent]\n",
    "selected_features = train_data_df.drop(drop_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### -----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3) OUTLIER DETECTION ##\n",
    "\n",
    "outlier_detect_sf = selected_features.copy()\n",
    "\n",
    "#specify the 12 metrics column names to be modelled\n",
    "to_model_columns = outlier_detect_sf.drop(['id', 'y'], axis=1).columns#[1:13]\n",
    "from sklearn.ensemble import IsolationForest\n",
    "clf=IsolationForest(n_estimators=100, max_samples='auto', contamination=float(.1), \\\n",
    "                        max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42, verbose=0)\n",
    "clf.fit(outlier_detect_sf[to_model_columns])\n",
    "pred = clf.predict(outlier_detect_sf[to_model_columns])\n",
    "outlier_detect_sf['anomaly']=pred\n",
    "outliers=outlier_detect_sf.loc[outlier_detect_sf['anomaly']==-1]\n",
    "outlier_index=list(outliers.index)\n",
    "#print(outlier_index)\n",
    "#Find the number of anomalies and normal points here points classified -1 are anomalous\n",
    "print(outlier_detect_sf['anomaly'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "pca = PCA(n_components=3)  # Reduce to k=3 dimensions\n",
    "scaler = StandardScaler()\n",
    "#normalize the metrics\n",
    "processed_train_df = train_data_df.copy()\n",
    "X = scaler.fit_transform(processed_train_df[to_model_columns])\n",
    "X_reduce = pca.fit_transform(X)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_zlabel(\"x_composite_3\")\n",
    "# Plot the compressed data points\n",
    "ax.scatter(X_reduce[:, 0], X_reduce[:, 1], zs=X_reduce[:, 2], s=4, lw=1, label=\"inliers\",c=\"green\")\n",
    "# Plot x's for the ground truth outliers\n",
    "ax.scatter(X_reduce[outlier_index,0],X_reduce[outlier_index,1], X_reduce[outlier_index,2],\n",
    "           lw=2, s=60, marker=\"x\", c=\"red\", label=\"outliers\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing possible outlier ratios\n",
    "ratios = [0.02,0.03,0.04,0.05,0.06,0.07,0.08]\n",
    "BMAC_dic = {}\n",
    "for ratio in ratios:\n",
    "    outlier_detect_sf = selected_features.copy()\n",
    "\n",
    "    #specify the 12 metrics column names to be modelled\n",
    "    to_model_columns = outlier_detect_sf.drop(['id', 'y'], axis=1).columns#[1:13]\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    clf=IsolationForest(n_estimators=100, max_samples='auto', contamination=float(ratio), \\\n",
    "                            max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42, verbose=0)\n",
    "    #clf=IsolationForest(n_estimators=100, max_samples='auto', contamination='auto',\n",
    "    #                        max_features=1.0, bootstrap=False, n_jobs=-1, random_state=42, verbose=0)\n",
    "    clf.fit(outlier_detect_sf[to_model_columns])\n",
    "    pred = clf.predict(outlier_detect_sf[to_model_columns])\n",
    "    outlier_detect_sf['anomaly']=pred\n",
    "    outliers=outlier_detect_sf.loc[outlier_detect_sf['anomaly']==-1]\n",
    "    outlier_index=list(outliers.index)\n",
    "\n",
    "    outlier_detect_sf_ = outlier_detect_sf[outlier_detect_sf['anomaly'] == 1]\n",
    "    outlier_detect_sf_ = outlier_detect_sf_.drop(['anomaly'], axis = 1)\n",
    "    splits = split_df(outlier_detect_sf_, 5)\n",
    "    BMACs = []\n",
    "\n",
    "    for i in range(5): # 5-fold cross-validation\n",
    "        train, test = make_train_and_test(splits, i)\n",
    "        X_train = train.drop(columns=['id','y']).to_numpy()\n",
    "        y_train = train['y'].to_numpy()\n",
    "        X_validate = test.drop(columns=['id','y']).to_numpy()\n",
    "        y_validate = test['y'].to_numpy()\n",
    "            \n",
    "        model = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
    "                                colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
    "                                importance_type='gain', learning_rate=0.1, max_delta_step=0,\n",
    "                                max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
    "                                n_jobs=1, nthread=None, objective='multi:softprob', random_state=0,\n",
    "                                reg_alpha=0, reg_lambda=1, seed=None,\n",
    "                                silent=None, subsample=1, verbosity=1)\n",
    "    \n",
    "        eval_set = [(X_validate, y_validate)]\n",
    "        #model.fit(X_train, y_train, early_stopping_rounds=50, eval_metric=\"rmse\", eval_set=eval_set, verbose=False)\n",
    "        model.fit(X_train, y_train)\n",
    "            \n",
    "        y_predicted = model.predict(X_validate)\n",
    "        current_BMAC = balanced_accuracy_score(y_validate, y_predicted)\n",
    "        BMACs.append(current_BMAC)\n",
    "        print('{i}th iteration done'.format(i=i+1))\n",
    "        print('----> BMAC score: {s}'.format(s=current_BMAC))\n",
    "\n",
    "    mean_BMAC = (1./len(BMACs)) * np.sum(BMACs)\n",
    "    print('------------------')\n",
    "    print('Mean BMAC score for number {n} is: {bmac}'.format(n=number, bmac=mean_BMAC))\n",
    "    BMAC_dic[number] = mean_BMAC\n",
    "print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
